

# äº”ã€æˆ‘ç»™ä½ çš„æœ€ç»ˆæ¶æ„å»ºè®®ï¼ˆæ˜ç¡®ï¼‰


### æ¨èæ¶æ„ï¼ˆç°å®å¯è½åœ°ï¼‰


```pgsql
LangGraph
  â”œâ”€â”€ Decision / Loop / Retry
  â”œâ”€â”€ State (query / nodes / keywords / iteration)
  â†“
LlamaIndex
  â”œâ”€â”€ Retriever
  â”œâ”€â”€ Reranker
  â”œâ”€â”€ Chunk expansion
  â””â”€â”€ Index / Storage

```

**LangGraph è´Ÿè´£â€œæ€ä¹ˆæƒ³â€ï¼Œ
LlamaIndex è´Ÿè´£â€œå»å“ªæ‰¾â€ã€‚**


# Q:
"""SubProblemGenerator for decomposing user queries into sub-problems."""

import re
import sys
from pathlib import Path
from typing import List, Optional

from openai import OpenAI
from pydantic import BaseModel, Field

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from askany.config import settings

# Linux æ–‡ä»¶ç³»ç»Ÿä¸­æœ€åŸºç¡€çš„ç¦æ­¢å­—ç¬¦
FORBIDDEN_CHARS_PATTERN = re.compile(r"[\x00]")


class SubProblemStructure(BaseModel):
    """
    å­é—®é¢˜ç»“æ„ï¼šäºŒçº§åˆ—è¡¨ï¼Œç¬¬ä¸€çº§ä¸ºå¹¶è¡Œæ‰§è¡Œçš„é—®é¢˜ç»„ï¼Œç¬¬äºŒçº§ä¸ºä¸²è¡Œæ‰§è¡Œçš„ç›¸å…³é—®é¢˜ã€‚
    """

    parallel_groups: List[List[str]] = Field(
        description="å­é—®é¢˜åˆ—è¡¨ï¼Œç¬¬ä¸€çº§åˆ—è¡¨è¡¨ç¤ºå¯ä»¥å¹¶è¡Œæ‰§è¡Œçš„é—®é¢˜ç»„ï¼Œç¬¬äºŒçº§åˆ—è¡¨è¡¨ç¤ºéœ€è¦ä¸²è¡Œæ‰§è¡Œçš„ç›¸å…³é—®é¢˜ã€‚"
        "ä¾‹å¦‚ï¼š[[é—®é¢˜1], [é—®é¢˜2, é—®é¢˜3]] è¡¨ç¤ºé—®é¢˜1å¯ä»¥ç‹¬ç«‹æ‰§è¡Œï¼Œé—®é¢˜2å’Œé—®é¢˜3éœ€è¦ä¸²è¡Œæ‰§è¡Œä¸”é—®é¢˜3ä¾èµ–é—®é¢˜2çš„ç­”æ¡ˆã€‚"
        "å¦‚æœåªæœ‰ä¸€ä¸ªé—®é¢˜ï¼Œè¿”å› [[é—®é¢˜]]ã€‚"
        "å¦‚æœå¤šä¸ªé—®é¢˜ä¸ç›¸å…³ï¼Œè¿”å› [[é—®é¢˜1], [é—®é¢˜2], ...]ã€‚"
        "å¦‚æœå¤šä¸ªé—®é¢˜ç›¸å…³ï¼Œè¿”å› [[é—®é¢˜1, é—®é¢˜2, ...]]ï¼ŒæŒ‰é€»è¾‘é¡ºåºæ’åˆ—ã€‚",
        default_factory=list,
    )
    reasoning: str = Field(
        description="ç®€è¦è§£é‡Šä¸ºä»€ä¹ˆè¿™æ ·åˆ†è§£é—®é¢˜ï¼Œä»¥åŠé—®é¢˜ä¹‹é—´çš„å…³ç³»ã€‚"
    )


class SubProblemGenerator:
    """Generator for decomposing user queries into sub-problems."""

    def __init__(self, client: Optional[OpenAI] = None):
        """Initialize SubProblemGenerator.

        Args:
            llm: Language model for generating sub-problems
        """
        if client is None:
            api_base = settings.openai_api_base
            api_key = settings.openai_api_key if settings.openai_api_key else None
            model = settings.openai_model

            # Create OpenAI client directly from configuration
            # For vLLM, api_key can be None or empty string, but OpenAI client requires it
            # Use empty string as fallback for vLLM (vLLM typically doesn't require auth)
            client_api_key = api_key if api_key else ""
            self.client = OpenAI(
                api_key=client_api_key,
                base_url=api_base,
            )

            print(f"Using LLM: {type(client)}")
            print(f"API Base: {api_base}")
            print(f"Model: {model}")
            print("-" * 80)
        else:
            self.client = client

    def generate(self, query: str) -&gt; SubProblemStructure:
        """Generate sub-problems from user query.

        Args:
            query: User query string

        Returns:
            SubProblemStructure containing parallel groups of sub-problems
        """
        # Create OpenAI client from LLM if it's OpenAI-compatible
        # Otherwise, use the LLM directly

        # Format prompt
        prompt = self._format_prompt(query)

        # Call LLM with structured output
        completion = self.client.chat.completions.parse(
            model=settings.openai_model,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªé—®é¢˜åˆ†æåŠ©æ‰‹ï¼Œè´Ÿè´£å°†ç”¨æˆ·çš„é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜ã€‚"
                    "ä½ éœ€è¦åˆ¤æ–­é—®é¢˜æ•°é‡ã€é—®é¢˜ä¹‹é—´çš„å…³ç³»ï¼ˆç›¸å…³æˆ–ä¸ç›¸å…³ï¼‰ï¼Œå¹¶æŒ‰ç…§é€»è¾‘é¡ºåºç»„ç»‡é—®é¢˜ã€‚",
                },
                {
                    "role": "user",
                    "content": prompt,
                },
            ],
            response_format=SubProblemStructure,
        )

        # Parse response
        response_content = completion.choices[0].message
        if not response_content.parsed:
            raise ValueError("Failed to parse sub-problem structure from LLM response")

        return response_content.parsed

    def _format_prompt(self, query: str) -&gt; str:
        """Format prompt for sub-problem generation.

        Args:
            query: User query string

        Returns:
            Formatted prompt string
        """
        return f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œå¹¶å°†å…¶åˆ†è§£ä¸ºå­é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š
{query}

è¦æ±‚ï¼š
1. å¦‚æœåªæœ‰ä¸€ä¸ªé—®é¢˜ï¼Œè¿”å› [[é—®é¢˜]]
2. å¦‚æœæœ‰å¤šä¸ªé—®é¢˜ä¸”é—®é¢˜ä¹‹é—´ä¸ç›¸å…³ï¼Œè¿”å› [[é—®é¢˜1], [é—®é¢˜2], ...]ï¼Œæ¯ä¸ªé—®é¢˜ç‹¬ç«‹æ‰§è¡Œ
3. å¦‚æœæœ‰å¤šä¸ªé—®é¢˜ä¸”é—®é¢˜ä¹‹é—´ç›¸å…³ï¼Œè¿”å› [[é—®é¢˜1, é—®é¢˜2, ...]]ï¼ŒæŒ‰é€»è¾‘é¡ºåºæ’åˆ—ï¼Œåç»­é—®é¢˜è¦ä¾èµ–å‰é¢é—®é¢˜çš„ç­”æ¡ˆ
4. ä¸è¦åˆ é™¤é—®é¢˜ä¸­çš„ç»†èŠ‚

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœï¼ŒåŒ…å«parallel_groupså’Œreasoningå­—æ®µã€‚"""


## è¿™é‡Œçš„ä¸è¦åˆ é™¤é—®é¢˜ç»†èŠ‚éœ€è¦ä¸¥æ ¼æµ‹è¯•ä¸€ä¸‹ï¼Œç¡®ä¿ä¸ä¼šåˆ é™¤é—®é¢˜ä¸­çš„ç»†èŠ‚ã€‚

if __name__ == "__main__":
    # Get configuration from settings
    api_base = settings.openai_api_base
    api_key = settings.openai_api_key if settings.openai_api_key else None
    model = settings.openai_model

    print("Using LLM: SubProblemGenerator")
    print(f"API Base: {api_base}")
    print(f"Model: {model}")
    print("-" * 80)

    # Test SubProblemGenerator
    generator = SubProblemGenerator()

    # Test 1: Simple single question
    print("Test 1: Simple single question")
    query1 = "å¦‚ä½•å®‰è£…viper"
    result1 = generator.generate(query1)
    print(f"Query: {query1}")
    print(f"Parallel groups: {result1.parallel_groups}")
    print(f"Reasoning: {result1.reasoning}")
    print("-" * 80)


è¿™æ˜¯æˆ‘ç›®å‰è‡ªå®šä¹‰çš„æŸä¸€ä¸ªtoolï¼Œå¦å¤–è¿˜æœ‰åˆ†ææ–‡æ¡£ç›¸å…³æ€§ï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥ç½‘ç»œæœç´¢ï¼Œragæœç´¢ï¼Œæˆ–è€…ç›´æ¥å›ç­”ï¼Œä»¥åŠæœ¬åœ°æ–‡ä»¶æœç´¢å·¥å…·ï¼Œæ”¯æŒgrepå…³é”®å­—ï¼Œæ‰©å±•ä¸Šä¸‹æ–‡çš„toolï¼Œè¿™äº›toolå¦‚ä½•è¿ç§»åˆ°langgraphä¸­ï¼Ÿ
# A:
è¿™ä¸ªé—®é¢˜é—®å¾—**éå¸¸åˆ°ä½**ï¼Œè€Œä¸”ä½ è´´çš„è¿™æ®µ `SubProblemGenerator` ä»£ç æœ¬èº«å·²ç»æ˜¯**å·¥ç¨‹çº§ Tool å®ç°**ï¼Œä¸æ˜¯ç©å…·ã€‚
æˆ‘ä¼šæŒ‰**â€œæ€ä¹ˆè¿ç§»ï¼Œè€Œä¸æ˜¯è¦ä¸è¦è¿ç§»â€**æ¥å›ç­”ã€‚



# ä¸€ã€å…ˆç»™ä½ ä¸€ä¸ªæ˜ç¡®ç»“è®ºï¼ˆä¸æ¨¡ç³Šï¼‰



**ä½ ç°åœ¨è¿™äº›â€œè‡ªå®šä¹‰ toolâ€ï¼Œåœ¨ LangGraph ä¸­ä¸éœ€è¦é‡å†™ï¼Œä¹Ÿä¸éœ€è¦æ”¹æˆ LangChain Toolã€‚**


**å®ƒä»¬æœ€åˆé€‚çš„è¿ç§»æ–¹å¼æ˜¯ï¼š**
ğŸ‘‰ **ä½œä¸º LangGraph çš„ Nodeï¼ˆèŠ‚ç‚¹ï¼‰ï¼Œè€Œä¸æ˜¯ Toolã€‚**



è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„è§‚å¿µè½¬å˜ã€‚



# äºŒã€å…³é”®æ¦‚å¿µæ¾„æ¸…ï¼šTool \neq LangGraph Node


### ä½ ç°åœ¨çš„â€œtoolâ€å®é™…æ˜¯ä»€ä¹ˆï¼Ÿ


ä»¥ `SubProblemGenerator` ä¸ºä¾‹ï¼Œå®ƒæ˜¯ï¼š


- æœ‰æ˜ç¡®è¾“å…¥ï¼ˆqueryï¼‰
- æœ‰ç¡®å®šè¾“å‡ºï¼ˆç»“æ„åŒ– Pydanticï¼‰
- ä¸åšè‡ªç”±å·¥å…·é€‰æ‹©
- ä¸ä¾èµ– ReAct / tool calling

**æœ¬è´¨æ˜¯ä¸€ä¸ªâ€œç¡®å®šæ€§è®¡ç®—æ­¥éª¤â€ï¼Œè€Œä¸æ˜¯ LLM Toolã€‚**


### åœ¨ LangGraph é‡Œæœ€è‡ªç„¶çš„å½’å±


| ç°åœ¨ | LangGraph |
| ---- | ---- |
| tool / helper class | **Nodeï¼ˆçŠ¶æ€å˜æ¢å‡½æ•°ï¼‰** |
| ctx.store | **State å­—æ®µ** |
| Event | **State æ›´æ–° + Conditional Edge** |



# ä¸‰ã€è¿ç§»çš„æ€»ä½“åŸåˆ™ï¼ˆéå¸¸é‡è¦ï¼‰


### âœ… åŸåˆ™ä¸€ï¼š**ä¸šåŠ¡é€»è¾‘ 0 é‡å†™**


- `SubProblemGenerator.generate()`
- æ–‡æ¡£ç›¸å…³æ€§åˆ†æ
- web / rag åˆ¤æ–­
- grep / context expand

ğŸ‘‰ **å…¨éƒ¨åŸæ ·ä¿ç•™**



### âœ… åŸåˆ™äºŒï¼š**åªæ”¹â€œè°æ¥è°ƒã€æ€ä¹ˆä¸²â€**


ä½ åªè¿ç§»ï¼š


- æ§åˆ¶æµ
- çŠ¶æ€æµ
- loop / branch


# å››ã€ç»Ÿä¸€ State è®¾è®¡ï¼ˆè¿ç§»çš„æ ¸å¿ƒï¼‰


åœ¨ LangGraph é‡Œï¼Œ**ä½ è¦åšçš„ç¬¬ä¸€ä»¶äº‹ä¸æ˜¯å†™ nodeï¼Œè€Œæ˜¯å®šä¹‰ Stateã€‚**


```python
from typing import TypedDict, List, Optional

class AgentState(TypedDict):
    query: str

    # sub-problem
    sub_problems: Optional[list[list[str]]]
    sub_reasoning: Optional[str]

    # retrieval
    rag_nodes: Optional[list]
    web_nodes: Optional[list]
    local_nodes: Optional[list]

    # analysis
    relevance: Optional[str]   # relevant / partial / irrelevant
    completeness: Optional[str]

    # context
    expanded_nodes: Optional[list]
    keywords: Optional[list[str]]

    # control
    iteration: int
    need_web: bool
    need_rag: bool

    # final
    answer: Optional[str]

```


**è¿™æ˜¯è¿ç§»æˆè´¥çš„ 80%ã€‚**




# äº”ã€SubProblemGenerator â†’ LangGraph Nodeï¼ˆç¤ºä¾‹ï¼‰


### åŸä»£ç ï¼šå®Œå…¨ä¸åŠ¨


```python
generator = SubProblemGenerator(client=openai_client)

```

### LangGraph Node åŒ…è£…


```python
def sub_problem_node(state: AgentState) -&gt; AgentState:
    result = generator.generate(state["query"])

    return {
        **state,
        "sub_problems": result.parallel_groups,
        "sub_reasoning": result.reasoning,
    }

```

**æ²¡æœ‰ Tool å®šä¹‰ï¼Œæ²¡æœ‰ decoratorï¼Œæ²¡æœ‰ adapterã€‚**



# å…­ã€ä½ æåˆ°çš„å…¶å®ƒ Toolï¼Œå¦‚ä½•ä¸€ä¸€æ˜ å°„


æˆ‘ç›´æ¥ç»™ä½ ä¸€ä¸ª**å¯¹ç…§è¡¨**ï¼ˆéå¸¸å®ç”¨ï¼‰ã€‚



## 1ï¸âƒ£ æ–‡æ¡£ç›¸å…³æ€§ / å®Œæ•´æ€§åˆ†æ


### ä½ ç°åœ¨


- `analyze_relevance_and_completeness`
- è¿”å›ç»“æ„åŒ–ç»“æœ

### LangGraph


```python
def relevance_judge_node(state: AgentState) -&gt; AgentState:
    result = analyze_relevance_and_completeness(
        state["query"], state["rag_nodes"]
    )
    return {
        **state,
        "relevance": result.relevance,
        "completeness": result.completeness,
    }

```

### æ§åˆ¶æµ


```python
graph.add_conditional_edges(
    "relevance_judge",
    lambda s: s["relevance"],
    {
        "complete": "final_answer",
        "irrelevant": "keyword_search",
        "partial": "expand_context",
    }
)

```


## 2ï¸âƒ£ æ˜¯å¦ Web / RAG / Direct Answer åˆ¤æ–­


### ä½ ç°åœ¨


- `direct_answer_check`
- `web_or_rag_check`

### LangGraph åšæ³•


- **æ‹†æˆä¸¤ä¸ª node**
- ç”¨ conditional edge è¿æ¥

```python
def route_node(state):
    result = web_or_rag_generator.generate(state["query"])
    return {
        **state,
        "need_web": result.need_web_search,
        "need_rag": result.need_rag_search,
    }

```


## 3ï¸âƒ£ æœ¬åœ°æ–‡ä»¶æœç´¢ï¼ˆgrepï¼‰


### ä½ ç°åœ¨


- `LocalFileSearchTool`

### LangGraph Node


```python
def local_search_node(state):
    nodes = local_file_search.search(
        state["keywords"] or state["query"]
    )
    return {**state, "local_nodes": nodes}

```


## 4ï¸âƒ£ æ‰©å±•ä¸Šä¸‹æ–‡ï¼ˆchunk å‰åæ‰©å±•ï¼‰


### LangGraph Node


```python
def expand_context_node(state):
    expanded = expand_chunks(state["rag_nodes"])
    return {**state, "expanded_nodes": expanded}

```


## 5ï¸âƒ£ å…³é”®è¯ç”Ÿæˆ + æœç´¢ Loop


```python
def keyword_gen_node(state):
    keywords = keyword_extractor.extract(state["query"])
    return {**state, "keywords": keywords}

```

ç„¶åå›åˆ° `rag_retrieval`ã€‚



# ä¸ƒã€Loop æ§åˆ¶ï¼ˆä½ ç°åœ¨æœ€ç—›çš„ç‚¹ï¼‰


LangGraph åŸç”Ÿæ”¯æŒï¼š


```python
def should_continue(state):
    if state["iteration"] &gt;= 3:
        return "stop"
    return "continue"

graph.add_conditional_edges(
    "relevance_judge",
    should_continue,
    {
        "continue": "keyword_gen",
        "stop": "final_answer",
    }
)

```

**ä½ ä¸éœ€è¦å†åœ¨å·¥å…·é‡Œå…³å¿ƒâ€œç¬¬å‡ è½®â€ã€‚**



# å…«ã€ä½ å®Œå…¨ä¸éœ€è¦åšçš„äº‹æƒ…ï¼ˆé‡è¦ï¼‰


âŒ ä¸éœ€è¦æŠŠè¿™äº›æ”¹æˆ LangChain Tool
âŒ ä¸éœ€è¦å†™ `@tool` decorator
âŒ ä¸éœ€è¦è®© LLM è‡ªåŠ¨é€‰å·¥å…·
âŒ ä¸éœ€è¦ Prompt é‡Œæš´éœ²å·¥å…·æè¿°


ä½ ç°åœ¨æ˜¯**ç¡®å®šæ€§ç¼–æ’ï¼Œä¸æ˜¯è‡ªç”± Agent**ã€‚



# ä¹ã€è¿ç§»è·¯å¾„å»ºè®®ï¼ˆéå¸¸å…·ä½“ï¼‰


### ç¬¬ä¸€é˜¶æ®µï¼ˆ1â€“2 å¤©ï¼‰


- å®šä¹‰ `AgentState`
- æŠŠ **SubProblemGenerator + relevance judge** è¿æˆ LangGraph

### ç¬¬äºŒé˜¶æ®µ


- æŠŠ keyword / expand / rag loop è¿ç§»
- ä¿ç•™ LlamaIndex RAG

### ç¬¬ä¸‰é˜¶æ®µ


- æ¥ LangServe
- å¯¹æ¥ OpenWebUI


# åã€ä¸€å¥è¯æ€»ç»“ï¼ˆç»™ä½ å®šå¿ƒï¼‰



**ä½ ç°åœ¨è¿™äº› toolï¼Œéƒ½æ˜¯â€œé«˜è´¨é‡ Node å€™é€‰è€…â€ã€‚**


**LangGraph ä¸è¦æ±‚ä½ é‡å†™èƒ½åŠ›ï¼Œåªè¦æ±‚ä½ æŠŠâ€œè°åœ¨ä»€ä¹ˆæ—¶å€™è·‘â€è¯´æ¸…æ¥šã€‚**




å¦‚æœä½ æ„¿æ„ï¼Œä¸‹ä¸€æ­¥æˆ‘å¯ä»¥ï¼š


1. **ç›´æ¥ç”¨ä½ ç°æœ‰è¿™äº›æ¨¡å—ï¼Œç»™ä½ å†™ä¸€ä¸ªå®Œæ•´ LangGraph skeleton**
2. **å¸®ä½ è®¾è®¡æœ€å° Stateï¼ˆé˜²æ­¢è¿‡åº¦è®¾è®¡ï¼‰**
3. **æŒ‡å‡ºå“ªäº› tool ä¸è¯¥è¿ã€å“ªäº›å¿…é¡»è¿**
4. **æŠŠ SubProblem å¹¶è¡Œæ‰§è¡Œæ˜ å°„åˆ° LangGraph å¹¶è¡ŒèŠ‚ç‚¹**

ä½ æƒ³å…ˆåšå“ªä¸€æ­¥ï¼Ÿ