# LangChain/LangGraph ä¼˜åŒ–ç‰¹æ€§æ¸…å•

åŸºäºæ–‡æ¡£è°ƒç ”å’Œé¡¹ç›®åˆ†æï¼Œä»¥ä¸‹æ˜¯é™¤äº† `LocalFileSearchTool` å’Œ `SummarizationMiddleware` ä¹‹å¤–ï¼Œå¯ä»¥æå‡æ•ˆæœæˆ–åŠŸèƒ½æ€§çš„ LangChain/LangGraph ç‰¹æ€§ã€‚

## ç›®å½•
1. [é”™è¯¯å¤„ç†å’Œé‡è¯•](#é”™è¯¯å¤„ç†å’Œé‡è¯•)
2. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
3. [å®‰å…¨å’Œéšç§](#å®‰å…¨å’Œéšç§)
4. [æµå¼å¤„ç†](#æµå¼å¤„ç†)
5. [å¹¶è¡Œå¤„ç†](#å¹¶è¡Œå¤„ç†)
6. [ç¼“å­˜æœºåˆ¶](#ç¼“å­˜æœºåˆ¶)
7. [ç›‘æ§å’Œå¯è§‚æµ‹æ€§](#ç›‘æ§å’Œå¯è§‚æµ‹æ€§)
8. [åŠ¨æ€è·¯ç”±å’Œå†³ç­–](#åŠ¨æ€è·¯ç”±å’Œå†³ç­–)
9. [äººæœºäº¤äº’](#äººæœºäº¤äº’)
10. [çŠ¶æ€ç®¡ç†ä¼˜åŒ–](#çŠ¶æ€ç®¡ç†ä¼˜åŒ–)

---

## 1. é”™è¯¯å¤„ç†å’Œé‡è¯•

### 1.1 ToolRetryMiddleware âš ï¸ ä¸é€‚ç”¨äºä½ çš„æ¶æ„

**åŠŸèƒ½**ï¼šè‡ªåŠ¨é‡è¯•å¤±è´¥çš„å·¥å…·è°ƒç”¨ï¼Œå¤„ç†ç½‘ç»œé”™è¯¯ã€è¶…æ—¶ç­‰ä¸´æ—¶æ•…éšœã€‚

**âš ï¸ é‡è¦è¯´æ˜**ï¼š
`ToolRetryMiddleware` æ˜¯ä¸º **Agent æ¨¡å¼**è®¾è®¡çš„ï¼ˆLLM è‡ªä¸»é€‰æ‹©å·¥å…·ï¼‰ï¼Œ**ä¸é€‚ç”¨äºä½ çš„æ¶æ„**ï¼

**ä¸ºä»€ä¹ˆä¸é€‚ç”¨**ï¼š
- `create_agent` ä¼š `bind_tools`ï¼Œæ‰€æœ‰å·¥å…·å®šä¹‰ä¼šæ±¡æŸ“æç¤ºè¯
- ä½ çš„èŠ‚ç‚¹æ˜¯**ç¡®å®šæ€§çš„**ï¼Œæ˜¾å¼è°ƒç”¨å·¥å…·ï¼ˆå¦‚ `web_search_tool.search(query)`ï¼‰ï¼Œä¸æ˜¯è®© LLM é€‰æ‹©å·¥å…·
- åœ¨åˆ†æç›¸å…³æ€§é˜¶æ®µï¼Œå¦‚æœç»‘å®šäº†å·¥å…·ï¼Œä¼šæ±¡æŸ“æç¤ºè¯ï¼ŒLLM å¯èƒ½è¯¯è°ƒç”¨å·¥å…·

**æ­£ç¡®çš„å®ç°æ–¹å¼**ï¼š
è¯·å‚è€ƒ ğŸ“„ [tool_retry_correct_implementation.md](./tool_retry_correct_implementation.md)

**æ¨èæ–¹æ¡ˆï¼šåœ¨å·¥å…·ç±»å†…éƒ¨å®ç°é‡è¯•**ï¼š
```python
# askany/workflow/WebSearchTool.py
class WebSearchTool:
    def search(self, query: str) -> List[NodeWithScore]:
        """æœç´¢ç½‘ç»œï¼ˆå¸¦è‡ªåŠ¨é‡è¯•ï¼‰"""
        last_exception = None
        delay = 1.0
        
        for attempt in range(3):  # max_retries
            try:
                return self._search_impl(query)
            except (ConnectionError, TimeoutError) as e:
                last_exception = e
                if attempt < 2:
                    logger.warning(f"Retrying in {delay}s...")
                    time.sleep(delay)
                    delay *= 2.0  # backoff_factor
                else:
                    logger.error("Failed after 3 attempts")
                    return []  # æˆ– raise
        return []
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- âœ… åœ¨ `WebSearchTool.search()` å†…éƒ¨å®ç°é‡è¯•ï¼ˆä½ å·²ç»åœ¨åšï¼‰
- âœ… åœ¨ `LocalFileSearchTool` å†…éƒ¨å®ç°é‡è¯•
- âœ… åœ¨ RAG æ£€ç´¢å†…éƒ¨å®ç°é‡è¯•
- âŒ **ä¸è¦**ä½¿ç”¨ `create_agent` + `ToolRetryMiddleware`

### 1.2 ModelRetryMiddleware âš ï¸ éœ€è¦é€‚é…ä½ çš„æ¶æ„

**åŠŸèƒ½**ï¼šè‡ªåŠ¨é‡è¯•å¤±è´¥çš„æ¨¡å‹è°ƒç”¨ï¼Œå¤„ç† API é™æµã€ä¸´æ—¶é”™è¯¯ç­‰ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- LLM API è°ƒç”¨å¤±è´¥ï¼ˆå¦‚ OpenAI API é™æµï¼‰
- vLLM æœåŠ¡ä¸´æ—¶ä¸å¯ç”¨
- ç½‘ç»œæŠ–åŠ¨å¯¼è‡´çš„æ¨¡å‹è°ƒç”¨å¤±è´¥

**âš ï¸ æ³¨æ„**ï¼š
`ModelRetryMiddleware` æ˜¯ä¸º `create_agent` è®¾è®¡çš„ã€‚å¦‚æœä½ çš„èŠ‚ç‚¹ç›´æ¥è°ƒç”¨ LLMï¼ˆä¸ä½¿ç”¨ `create_agent`ï¼‰ï¼Œéœ€è¦é€‚é…ã€‚

**æ–¹æ¡ˆ Aï¼šä½¿ç”¨ wrap_model_callï¼ˆæ¨èï¼‰**
```python
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain_openai import ChatOpenAI
import time

@wrap_model_call
def model_retry_middleware(request: ModelRequest, handler) -> ModelResponse:
    """è‡ªå®šä¹‰æ¨¡å‹é‡è¯•ä¸­é—´ä»¶"""
    max_retries = 3
    backoff_factor = 2.0
    initial_delay = 1.0
    retry_on = (ConnectionError, TimeoutError, APIError)
    
    last_exception = None
    delay = initial_delay
    
    for attempt in range(max_retries):
        try:
            return handler(request)
        except retry_on as e:
            last_exception = e
            if attempt < max_retries - 1:
                logger.warning(f"Model call failed, retrying in {delay}s...")
                time.sleep(delay)
                delay *= backoff_factor
            else:
                logger.error("Model call failed after all retries")
                raise
    
    if last_exception:
        raise last_exception

# åœ¨èŠ‚ç‚¹ä¸­ä½¿ç”¨
def analyze_relevance_node(state: AgentState) -> AgentState:
    """åˆ†æç›¸å…³æ€§èŠ‚ç‚¹ï¼ˆå¸¦é‡è¯•ï¼‰"""
    model = ChatOpenAI(model="gpt-4o")
    model_with_retry = model.with_config({"middleware": [model_retry_middleware]})
    # ä½¿ç”¨ model_with_retry è°ƒç”¨
```

**æ–¹æ¡ˆ Bï¼šåœ¨èŠ‚ç‚¹å‡½æ•°å†…éƒ¨å®ç°é‡è¯•ï¼ˆæ›´ç®€å•ï¼‰**
```python
def analyze_relevance_and_completeness(query, nodes, client):
    """åˆ†æç›¸å…³æ€§ï¼ˆå¸¦é‡è¯•ï¼‰"""
    max_retries = 3
    delay = 1.0
    
    for attempt in range(max_retries):
        try:
            completion = client.chat.completions.parse(...)
            return completion.choices[0].message.parsed
        except (ConnectionError, TimeoutError, APIError) as e:
            if attempt < max_retries - 1:
                logger.warning(f"LLM call failed, retrying in {delay}s...")
                time.sleep(delay)
                delay *= 2.0
            else:
                logger.error("LLM call failed after all retries")
                raise
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- âœ… åœ¨ `analyze_relevance_and_completeness` ä¸­æ·»åŠ é‡è¯•
- âœ… åœ¨ `DirectAnswerGenerator.generate` ä¸­æ·»åŠ é‡è¯•
- âœ… åœ¨ `SubProblemGenerator.generate` ä¸­æ·»åŠ é‡è¯•
- âœ… å¯ä»¥æ›¿æ¢ `AutoRetryVLLM` ä¸­çš„æ‰‹åŠ¨é‡è¯•é€»è¾‘

### 1.3 LangGraph èŠ‚ç‚¹çº§é‡è¯•ç­–ç•¥
**åŠŸèƒ½**ï¼šåœ¨ LangGraph èŠ‚ç‚¹çº§åˆ«é…ç½®é‡è¯•ç­–ç•¥ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langgraph.graph import StateGraph

workflow = StateGraph(AgentState)
workflow.add_node(
    "rag_retrieval",
    rag_retrieval_node,
    retry_policy={
        "max_attempts": 3,
        "initial_delay": 1.0,
        "backoff_factor": 2.0,
    }
)
```

---

## 2. æ€§èƒ½ä¼˜åŒ–

### 2.1 Batch æ‰¹å¤„ç†
**åŠŸèƒ½**ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªç‹¬ç«‹è¯·æ±‚ï¼Œæé«˜ååé‡å’Œé™ä½æˆæœ¬ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- å¹¶è¡Œå¤„ç†å¤šä¸ªå­é—®é¢˜
- æ‰¹é‡æ£€ç´¢å¤šä¸ªæ–‡æ¡£
- æ‰¹é‡åˆ†æå¤šä¸ªæŸ¥è¯¢çš„ç›¸å…³æ€§

**å®ç°ç¤ºä¾‹**ï¼š
```python
# æ‰¹é‡å¤„ç†å¤šä¸ªæŸ¥è¯¢
queries = [
    "å¦‚ä½•æ›´æ–°ç»„ä»¶ï¼Ÿ",
    "å¦‚ä½•æŸ¥çœ‹æ—¥å¿—ï¼Ÿ"
]

# æ‰¹é‡è°ƒç”¨æ¨¡å‹
responses = model.batch(queries)
for response in responses:
    print(response.content)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- åœ¨ `process_parallel_group` ä¸­ä½¿ç”¨ batch å¤„ç†å¹¶è¡Œå­é—®é¢˜
- æ‰¹é‡åˆ†æå¤šä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§

### 2.2 Prompt Cachingï¼ˆæç¤ºè¯ç¼“å­˜ï¼‰
**åŠŸèƒ½**ï¼šç¼“å­˜é‡å¤çš„æç¤ºè¯éƒ¨åˆ†ï¼Œé™ä½å»¶è¿Ÿå’Œæˆæœ¬ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- ç³»ç»Ÿæç¤ºè¯é‡å¤ä½¿ç”¨
- æ–‡æ¡£æ¨¡æ¿é‡å¤ä½¿ç”¨
- å›ºå®šæ ¼å¼çš„æç¤ºè¯

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain_openai import ChatOpenAI

# OpenAI è‡ªåŠ¨ç¼“å­˜ï¼ˆéšå¼ï¼‰
model = ChatOpenAI(model="gpt-4o")

# Anthropic æ˜¾å¼ç¼“å­˜
from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware

model = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    middleware=[AnthropicPromptCachingMiddleware()]
)

# ä½¿ç”¨ prompt_cache_key æ ‡è®°å¯ç¼“å­˜éƒ¨åˆ†
messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè¿ç»´åŠ©æ‰‹"},  # å¯ç¼“å­˜
    {"role": "user", "content": query}  # ä¸ç¼“å­˜
]
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- ç¼“å­˜ `DirectAnswerGenerator` å’Œ `WebOrRagAnswerGenerator` çš„ç³»ç»Ÿæç¤ºè¯
- ç¼“å­˜ `SubProblemGenerator` çš„æç¤ºè¯æ¨¡æ¿

### 2.3 åŠ¨æ€æ¨¡å‹é€‰æ‹©
**åŠŸèƒ½**ï¼šæ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€é€‰æ‹©æ¨¡å‹ï¼ˆå°æ¨¡å‹å¤„ç†ç®€å•ä»»åŠ¡ï¼Œå¤§æ¨¡å‹å¤„ç†å¤æ‚ä»»åŠ¡ï¼‰ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from langchain_openai import ChatOpenAI

basic_model = ChatOpenAI(model="gpt-4o-mini")
advanced_model = ChatOpenAI(model="gpt-4o")

@wrap_model_call
def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:
    """æ ¹æ®å¯¹è¯å¤æ‚åº¦é€‰æ‹©æ¨¡å‹"""
    message_count = len(request.state["messages"])
    query_complexity = estimate_complexity(request.state["messages"][-1].content)
    
    if message_count > 10 or query_complexity > 0.7:
        model = advanced_model
    else:
        model = basic_model
    
    return handler(request.override(model=model))

agent = create_agent(
    model=basic_model,
    tools=[...],
    middleware=[dynamic_model_selection]
)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- ç®€å•é—®é¢˜ä½¿ç”¨ `gpt-4o-mini`ï¼Œå¤æ‚é—®é¢˜ä½¿ç”¨ `gpt-4o`
- æ ¹æ®æŸ¥è¯¢ç±»å‹é€‰æ‹©ä¸åŒæ¨¡å‹

---

## 3. å®‰å…¨å’Œéšç§

### 3.1 PIIMiddlewareï¼ˆä¸ªäººèº«ä»½ä¿¡æ¯ä¸­é—´ä»¶ï¼‰
**åŠŸèƒ½**ï¼šè‡ªåŠ¨æ£€æµ‹å’Œè„±æ•æ•æ„Ÿä¿¡æ¯ï¼ˆé‚®ç®±ã€ä¿¡ç”¨å¡å·ã€IPåœ°å€ç­‰ï¼‰ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤„ç†ç”¨æˆ·è¾“å…¥ä¸­çš„æ•æ„Ÿä¿¡æ¯
- æ—¥å¿—è®°å½•å‰è„±æ•
- ç¬¦åˆéšç§æ³•è§„è¦æ±‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain.agents.middleware import PIIMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[...],
    middleware=[
        PIIMiddleware(
            strategy="redact",  # æˆ– "block"
            pii_types=["email", "credit_card", "ip_address"],
        ),
    ],
)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- åœ¨ API å…¥å£å¤„æ·»åŠ  PII æ£€æµ‹
- æ—¥å¿—è®°å½•å‰è„±æ•æ•æ„Ÿä¿¡æ¯

---

## 4. æµå¼å¤„ç†

### 4.1 Streaming æµå¼å“åº”
**åŠŸèƒ½**ï¼šå®æ—¶æµå¼è¿”å›ç»“æœï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langgraph.graph import StateGraph

# æµå¼è°ƒç”¨
for chunk in graph.stream({"messages": "..."}, stream_mode="updates"):
    print(chunk)

# æµå¼äº‹ä»¶
for event in graph.stream_events({"messages": "..."}, version="v2"):
    print(event)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- LangServe æ¥å£æ”¯æŒæµå¼è¿”å›
- å®æ—¶æ˜¾ç¤ºæ£€ç´¢è¿›åº¦
- å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿‡ç¨‹

### 4.2 Stream Mode é…ç½®
**åŠŸèƒ½**ï¼šæ§åˆ¶æµå¼è¾“å‡ºçš„ç²’åº¦ã€‚

**é€‰é¡¹**ï¼š
- `"values"`: æ¯æ¬¡çŠ¶æ€æ›´æ–°
- `"messages"`: æ¯æ¬¡æ¶ˆæ¯æ›´æ–°
- `"updates"`: æ¯ä¸ªèŠ‚ç‚¹æ›´æ–°

---

## 5. å¹¶è¡Œå¤„ç†

### 5.1 LangGraph å¹¶è¡ŒèŠ‚ç‚¹
**åŠŸèƒ½**ï¼šåœ¨ LangGraph ä¸­å®ç°çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langgraph.graph import StateGraph, START, END

workflow = StateGraph(AgentState)

# æ·»åŠ å¹¶è¡ŒèŠ‚ç‚¹
workflow.add_node("keyword_search", keyword_search_node)
workflow.add_node("hypothetical_search", hypothetical_search_node)
workflow.add_node("web_search", web_search_node)

# ä» START å¹¶è¡Œæ‰§è¡Œ
workflow.add_edge(START, "keyword_search")
workflow.add_edge(START, "hypothetical_search")
workflow.add_edge(START, "web_search")

# èšåˆèŠ‚ç‚¹ç­‰å¾…æ‰€æœ‰å¹¶è¡ŒèŠ‚ç‚¹å®Œæˆ
workflow.add_node("aggregate", aggregate_node)
workflow.add_edge("keyword_search", "aggregate")
workflow.add_edge("hypothetical_search", "aggregate")
workflow.add_edge("web_search", "aggregate")
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- æ›¿æ¢ `_concurrent_search` ä¸­çš„ `asyncio.gather`
- å®ç°çœŸæ­£çš„å¹¶è¡Œæ£€ç´¢ï¼ˆå…³é”®è¯æ£€ç´¢ + å‡è®¾ç­”æ¡ˆæ£€ç´¢ï¼‰

---

## 6. ç¼“å­˜æœºåˆ¶

### 6.1 Semantic Cacheï¼ˆè¯­ä¹‰ç¼“å­˜ï¼‰
**åŠŸèƒ½**ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ç¼“å­˜æŸ¥è¯¢ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- ç›¸ä¼¼æŸ¥è¯¢çš„ç¼“å­˜
- é™ä½é‡å¤æ£€ç´¢æˆæœ¬
- æé«˜å“åº”é€Ÿåº¦

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain.cache import SemanticCache
from langchain_openai import OpenAIEmbeddings

cache = SemanticCache(
    embedding=OpenAIEmbeddings(),
    similarity_threshold=0.8,  # ç›¸ä¼¼åº¦é˜ˆå€¼
)

# ä½¿ç”¨ç¼“å­˜
from langchain.globals import set_llm_cache
set_llm_cache(cache)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- ç¼“å­˜ç›¸ä¼¼æŸ¥è¯¢çš„ RAG æ£€ç´¢ç»“æœ
- ç¼“å­˜ç›¸å…³æ€§åˆ†æç»“æœ

### 6.2 In-Memory Cache
**åŠŸèƒ½**ï¼šå†…å­˜ç¼“å­˜ï¼Œé€‚åˆå¼€å‘æµ‹è¯•ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())
```

---

## 7. ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### 7.1 LangSmith é›†æˆ
**åŠŸèƒ½**ï¼šå®Œæ•´çš„è¿½è¸ªã€ç›‘æ§å’Œè°ƒè¯•èƒ½åŠ›ã€‚

**åŠŸèƒ½ç‰¹æ€§**ï¼š
- è¯·æ±‚è¿½è¸ª
- æ€§èƒ½ç›‘æ§
- æˆæœ¬åˆ†æ
- é”™è¯¯è¿½è¸ª
- æç¤ºè¯ç‰ˆæœ¬ç®¡ç†

**å®ç°ç¤ºä¾‹**ï¼š
```python
import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-api-key"
os.environ["LANGCHAIN_PROJECT"] = "askany-workflow"
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- è¿½è¸ªæ‰€æœ‰ LLM è°ƒç”¨
- åˆ†ææ€§èƒ½ç“¶é¢ˆ
- ç›‘æ§æˆæœ¬
- è°ƒè¯•é—®é¢˜

### 7.2 è‡ªå®šä¹‰è¿½è¸ª
**åŠŸèƒ½**ï¼šæ·»åŠ è‡ªå®šä¹‰è¿½è¸ªç‚¹ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain_core.tracers import LangChainTracer

tracer = LangChainTracer()
# è‡ªåŠ¨è¿½è¸ªæ‰€æœ‰è°ƒç”¨
```

---

## 8. åŠ¨æ€è·¯ç”±å’Œå†³ç­–

### 8.1 Conditional Edgesï¼ˆæ¡ä»¶è¾¹ï¼‰
**åŠŸèƒ½**ï¼šæ ¹æ®çŠ¶æ€åŠ¨æ€å†³å®šä¸‹ä¸€æ­¥æ‰§è¡Œå“ªä¸ªèŠ‚ç‚¹ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langgraph.graph import StateGraph, START, END

def should_use_rag(state: AgentState) -> str:
    """æ ¹æ®çŠ¶æ€å†³å®šæ˜¯å¦ä½¿ç”¨ RAG"""
    if state.get("need_rag_search"):
        return "rag_retrieval"
    elif state.get("need_web_search"):
        return "web_search"
    else:
        return "direct_answer"

workflow = StateGraph(AgentState)
workflow.add_node("classify", classify_node)
workflow.add_conditional_edges(
    "classify",
    should_use_rag,
    {
        "rag_retrieval": "rag_node",
        "web_search": "web_node",
        "direct_answer": "answer_node",
    }
)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- æ›¿æ¢ `direct_answer_check` å’Œ `web_or_rag_check` çš„é€»è¾‘
- å®ç°æ›´æ¸…æ™°çš„æ¡ä»¶è·¯ç”±

### 8.2 Command å¯¹è±¡
**åŠŸèƒ½**ï¼šåœ¨å·¥å…·ä¸­åŠ¨æ€æ§åˆ¶å›¾æ‰§è¡Œæµç¨‹ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langgraph.types import Command
from langchain.tools import tool

@tool
def dynamic_reroute(query: str) -> Command:
    """æ ¹æ®æŸ¥è¯¢åŠ¨æ€å†³å®šè·¯ç”±"""
    if "ç´§æ€¥" in query:
        return Command(
            update={"priority": "high"},
            goto="urgent_handler"
        )
    return Command(update={"priority": "normal"})
```

---

## 9. äººæœºäº¤äº’

### 9.1 HumanInTheLoopMiddleware
**åŠŸèƒ½**ï¼šåœ¨æ•æ„Ÿæ“ä½œå‰æš‚åœï¼Œç­‰å¾…äººå·¥å®¡æ ¸ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- æ•°æ®åº“å†™æ“ä½œ
- æ–‡ä»¶åˆ é™¤æ“ä½œ
- æ•æ„Ÿä¿¡æ¯æŸ¥è¯¢

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model="gpt-4o",
    tools=[database_write_tool, file_delete_tool],
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={
                "database_write_tool": True,
                "file_delete_tool": True,
            },
            description_prefix="æ“ä½œç­‰å¾…å®¡æ ¸",
        ),
    ],
    checkpointer=InMemorySaver(),
)

# æ‰§è¡Œæ—¶ä¼šæš‚åœç­‰å¾…å®¡æ ¸
result = agent.invoke({"messages": "åˆ é™¤æ–‡ä»¶ X"}, config)
# å®¡æ ¸é€šè¿‡åç»§ç»­
result = agent.invoke(None, config)  # ç»§ç»­æ‰§è¡Œ
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- æ•æ„ŸæŸ¥è¯¢çš„äººå·¥å®¡æ ¸
- å±é™©æ“ä½œå‰çš„ç¡®è®¤

---

## 10. çŠ¶æ€ç®¡ç†ä¼˜åŒ–

### 10.1 Checkpointer æŒä¹…åŒ–
**åŠŸèƒ½**ï¼šæŒä¹…åŒ–çŠ¶æ€ï¼Œæ”¯æŒæ¢å¤å’Œè°ƒè¯•ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langgraph.checkpoint.postgres import PostgresSaver

# PostgreSQL æŒä¹…åŒ–ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
checkpointer = PostgresSaver.from_conn_string(
    "postgresql://user:pass@localhost/dbname"
)

# SQLite æŒä¹…åŒ–ï¼ˆæœ¬åœ°å¼€å‘ï¼‰
from langgraph.checkpoint.sqlite import SqliteSaver
checkpointer = SqliteSaver.from_conn_string("sqlite:///checkpoints.db")

graph = workflow.compile(checkpointer=checkpointer)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- ä¼šè¯çŠ¶æ€æŒä¹…åŒ–
- é”™è¯¯æ¢å¤
- è°ƒè¯•å’Œå®¡è®¡

### 10.2 State æ›´æ–°ä¼˜åŒ–
**åŠŸèƒ½**ï¼šä½¿ç”¨ Command ç²¾ç¡®æ§åˆ¶çŠ¶æ€æ›´æ–°ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langgraph.types import Command
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES

@tool
def clear_history() -> Command:
    """æ¸…ç©ºå¯¹è¯å†å²"""
    return Command(
        update={
            "messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
        }
    )
```

---

## 11. Rate Limitingï¼ˆé€Ÿç‡é™åˆ¶ï¼‰

### 11.1 InMemoryRateLimiter
**åŠŸèƒ½**ï¼šé™åˆ¶ API è°ƒç”¨é€Ÿç‡ï¼Œé¿å…è§¦å‘é™æµã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain_core.rate_limiters import InMemoryRateLimiter
from langchain_community.tools import TavilySearchResults

# é™åˆ¶ä¸ºæ¯ç§’ 0.1 æ¬¡è¯·æ±‚
rate_limiter = InMemoryRateLimiter(requests_per_second=0.1)

tool = TavilySearchResults(rate_limiter=rate_limiter)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- WebSearchTool çš„é€Ÿç‡é™åˆ¶
- LLM API è°ƒç”¨çš„é€Ÿç‡é™åˆ¶

---

## 12. æ¶ˆæ¯å…ƒæ•°æ®

### 12.1 Message Metadata
**åŠŸèƒ½**ï¼šä¸ºæ¶ˆæ¯æ·»åŠ å…ƒæ•°æ®ï¼Œæ”¯æŒè¿½è¸ªå’Œè·¯ç”±ã€‚

**å®ç°ç¤ºä¾‹**ï¼š
```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content="æŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯",
    name="alice",  # ç”¨æˆ·æ ‡è¯†
    id="msg_123",  # æ¶ˆæ¯ID
    metadata={
        "user_id": "user_123",
        "session_id": "session_456",
        "priority": "high",
    }
)
```

**åœ¨ä½ çš„é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š
- è¿½è¸ªç”¨æˆ·ä¼šè¯
- è·¯ç”±ä¸åŒç”¨æˆ·çš„æ¶ˆæ¯
- ä¼˜å…ˆçº§å¤„ç†

---

## å®æ–½ä¼˜å…ˆçº§å»ºè®®

### é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰
1. **ToolRetryMiddleware** - æå‡ç³»ç»Ÿç¨³å®šæ€§
2. **ModelRetryMiddleware** - ç»Ÿä¸€é”™è¯¯å¤„ç†
3. **Checkpointer æŒä¹…åŒ–** - æ”¯æŒä¼šè¯å’Œæ¢å¤
4. **LangSmith é›†æˆ** - ç›‘æ§å’Œè°ƒè¯•

### ä¸­ä¼˜å…ˆçº§ï¼ˆè¿‘æœŸå®æ–½ï¼‰
5. **SummarizationMiddleware** - ç®¡ç†é•¿å¯¹è¯
6. **Batch æ‰¹å¤„ç†** - æå‡å¹¶è¡Œå¤„ç†æ€§èƒ½
7. **Prompt Caching** - é™ä½æˆæœ¬
8. **Streaming** - æå‡ç”¨æˆ·ä½“éªŒ

### ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸä¼˜åŒ–ï¼‰
9. **PIIMiddleware** - éšç§ä¿æŠ¤
10. **HumanInTheLoopMiddleware** - æ•æ„Ÿæ“ä½œå®¡æ ¸
11. **Semantic Cache** - ç¼“å­˜ä¼˜åŒ–
12. **åŠ¨æ€æ¨¡å‹é€‰æ‹©** - æˆæœ¬ä¼˜åŒ–

---

## å‚è€ƒèµ„æº

- [LangChain Middleware æ–‡æ¡£](https://docs.langchain.com/oss/python/langchain/middleware/built-in)
- [LangGraph æ–‡æ¡£](https://docs.langchain.com/oss/python/langgraph)
- [LangSmith æ–‡æ¡£](https://docs.langchain.com/langsmith)
- [LangChain v1.0 å‘å¸ƒè¯´æ˜](https://blog.langchain.com/langchain-langgraph-1dot0/)
