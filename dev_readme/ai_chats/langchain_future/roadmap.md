- æ”¯æŒå¤šç§æ¨¡å¼
from langchain_openai import ChatOpenAI
from langchain.agents import create_agent
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse


basic_model = ChatOpenAI(model="gpt-4o-mini")
advanced_model = ChatOpenAI(model="gpt-4o")

@wrap_model_call
def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:
    """Choose model based on conversation complexity."""
    message_count = len(request.state["messages"])

    if message_count > 10:
        # Use an advanced model for longer conversations
        model = advanced_model
    else:
        model = basic_model

    return handler(request.override(model=model))

agent = create_agent(
    model=basic_model,  # Default model
    tools=tools,
    middleware=[dynamic_model_selection]
)


- 

Batch
https://docs.langchain.com/oss/python/langchain/models#batch
Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:
Batch
responses = model.batch([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
])
for response in responses:
    print(response)
batch æ”¯æŒ


Prompt caching
Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be implicit or explicit:
Implicit prompt caching: providers will automatically pass on cost savings if a request hits a cache. Examples: OpenAI and Gemini.
Explicit caching: providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:
ChatOpenAI (via prompt_cache_key)
Anthropicâ€™s AnthropicPromptCachingMiddleware
Gemini.
AWS Bedrock





Message metadata
Add metadata
human_msg = HumanMessage(
    content="Hello!",
    name="alice",  # Optional: identify different users
    id="msg_123",  # Optional: unique identifier for tracing
)

èŠå¤©å®¤æ¥å…¥robotï¼Ÿ


Updating state:
Use Command to update the agentâ€™s state or control the graphâ€™s execution flow:
from langgraph.types import Command
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langchain.tools import tool, ToolRuntime

# Update the conversation history by removing all messages
@tool
def clear_conversation() -> Command:
    """Clear the conversation history."""

    return Command(
        update={
            "messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
        }
    )

# Update the user_name in the agent state
@tool
def update_user_name(
    new_name: str,
    runtime: ToolRuntime
) -> Command:
    """Update the user's name."""
    return Command(update={"user_name": new_name})
å‹ç¼©ä¸Šä¸‹æ–‡ åŒæ—¶ç›¸å…³çš„ï¼š short-memory checkpoint
To summarize message history in an agent, use the built-in SummarizationMiddleware:

from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.runnables import RunnableConfig


checkpointer = InMemorySaver()

agent = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        SummarizationMiddleware(
            model="gpt-4o-mini",
            trigger=("tokens", 4000),
            keep=("messages", 20)
        )
    ],
    checkpointer=checkpointer,
)

config: RunnableConfig = {"configurable": {"thread_id": "1"}}
agent.invoke({"messages": "hi, my name is bob"}, config)
agent.invoke({"messages": "write a short poem about cats"}, config)
agent.invoke({"messages": "now do the same but for dogs"}, config)
final_response = agent.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
"""


ClearToolUsesEdit


================================== Ai Message ==================================

Your name is Bob!
"""


TodoListMiddleware

ToolRetryMiddleware
ModelRetryMiddleware


https://docs.langchain.com/oss/python/langchain/middleware/built-in#file-search å¯ä»¥åœ¨æŸä¸ªèŠ‚ç‚¹ä½¿ç”¨å— LocalFileSearchToolä¸€è½®æ‰¾ä¸åˆ°å°±ç”¨è¿™ä¸ªï¼Ÿ


human in the loop

---

## å®Œæ•´ä¼˜åŒ–ç‰¹æ€§æ¸…å•

è¯¦ç»†çš„ LangChain/LangGraph ä¼˜åŒ–ç‰¹æ€§è¯´æ˜ï¼Œè¯·å‚è€ƒï¼š
ğŸ“„ [optimization_features.md](./optimization_features.md)

### å¿«é€Ÿå‚è€ƒ

**é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰**ï¼š
- âœ… **ToolRetryMiddleware** - è‡ªåŠ¨é‡è¯•å¤±è´¥çš„å·¥å…·è°ƒç”¨
- âœ… **ModelRetryMiddleware** - è‡ªåŠ¨é‡è¯•å¤±è´¥çš„æ¨¡å‹è°ƒç”¨
- âœ… **Checkpointer æŒä¹…åŒ–** - PostgreSQL/SQLite çŠ¶æ€æŒä¹…åŒ–
- âœ… **LangSmith é›†æˆ** - å®Œæ•´çš„è¿½è¸ªå’Œç›‘æ§

**ä¸­ä¼˜å…ˆçº§ï¼ˆè¿‘æœŸå®æ–½ï¼‰**ï¼š
- âœ… **SummarizationMiddleware** - ç®¡ç†é•¿å¯¹è¯ä¸Šä¸‹æ–‡
- âœ… **Batch æ‰¹å¤„ç†** - å¹¶è¡Œå¤„ç†å¤šä¸ªè¯·æ±‚
- âœ… **Prompt Caching** - é™ä½æˆæœ¬å’Œå»¶è¿Ÿ
- âœ… **Streaming** - å®æ—¶æµå¼å“åº”

**ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸä¼˜åŒ–ï¼‰**ï¼š
- âœ… **PIIMiddleware** - æ•æ„Ÿä¿¡æ¯è„±æ•
- âœ… **HumanInTheLoopMiddleware** - æ•æ„Ÿæ“ä½œå®¡æ ¸
- âœ… **Semantic Cache** - è¯­ä¹‰ç›¸ä¼¼åº¦ç¼“å­˜
- âœ… **åŠ¨æ€æ¨¡å‹é€‰æ‹©** - æ ¹æ®å¤æ‚åº¦é€‰æ‹©æ¨¡å‹
- âœ… **Rate Limiting** - API è°ƒç”¨é€Ÿç‡é™åˆ¶
- âœ… **å¹¶è¡ŒèŠ‚ç‚¹** - LangGraph çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œ
